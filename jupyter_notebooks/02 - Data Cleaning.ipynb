{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# **Trial for first commit**"
   ]
  },
  {
   "source": [
    "# **Data Cleaning Notebook**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "* Evaluate and manage the missing data\n",
    "* Clean data\n",
    "        \n",
    "## Inputs\n",
    "        \n",
    "* outputs/datasets/collection/HousePricesData.csv\n",
    "        \n",
    "## Outputs\n",
    "        \n",
    "* Generate cleaned Train and Test sets, both saved under outputs/datasets/cleaned\n",
    "* Data cleaning pipeline\n",
    "\n",
    "## Additional comments \n",
    "\n",
    "* We will only be dealing with the HousePricesData dataset as the inherited houses dataset does not require analysis, it will only be used to make predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqP-UeN-z3i2"
   },
   "source": [
    "## Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
   ]
  },
  {
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ],
   "cell_type": "markdown",
   "metadata": {
    "id": "aOGIGS-uz3i2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZfF_j-Bz3i4",
    "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MWW8E7lz3i7"
   },
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwHsQRWjz3i9",
    "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_xPk_Ijz3i-"
   },
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vz3S-_kjz3jA",
    "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mavJ8DibrcQ"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the pandas library\n",
    "* Load the dataset as a pandas DataFrame and assign it to our dataframe df_prices\n",
    "* View the data in the df_prices variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_prices = (pd.read_csv(\"outputs/datasets/collection/HousePricesData.csv\"))\n",
    "df_prices.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY3l0-AxO93d"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFQo3ycuO-v6"
   },
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are interested in delving deeper into the database. We want to identify the missing data, check the distribution and shape of the variables linked to said missing data. \n",
    "\n",
    "* First, we will identify the variables containing missing data:\n",
    "    * We create a list containing all columns with missing data\n",
    "    * We check it's length\n",
    "    * We show the variables containing missing data\n",
    "    * We show the count and type of missing data for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing_data = df_prices.columns[df_prices.isna().sum() > 0].to_list()\n",
    "len(vars_with_missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices[vars_with_missing_data].info()"
   ]
  },
  {
   "source": [
    "* We will now use panda's profiling report to get an overview of the missing data as well as detailed information for each variable. Here we are only considering the variables containing missing data, **not** all variables of our dataset.\n",
    "    * We notice that 26% of our data is missing\n",
    "    * We have both numerical and categorical data missing\n",
    "    * In the \"alerts\" section, we notice that two variables have very high levels of missing data. They are \"EnclosedPorch\" and \"WoodDeckSF\" with 90.7% and 89.4% of missing data respectively.\n",
    "    * In the same section, we see that the variables \"2ndFlrSF\" and \"MasVnrArea\" have 53.5% and 59% of zeros respectively."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "pandas_report = ProfileReport(df=df_prices[vars_with_missing_data], minimal=True)\n",
    "pandas_report.to_notebook_iframe()"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data cleaning\n",
    "\n",
    "### Preparation\n",
    "\n",
    "We will create a custom function to visualize missing data levels. It will show the data type, how many rows are missing and what percentage it represents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VisualizeMissingData(df):\n",
    "  \"\"\"\n",
    "  Function to visualize missing values levels within dataframe\n",
    "  \"\"\"\n",
    "  missing_data_absolute = df.isnull().sum()\n",
    "  missing_data_percentage = round(missing_data_absolute/len(df)*100 , 2)\n",
    "  df_missing_data = (pd.DataFrame(\n",
    "                          data= {\"DataType\":df.dtypes,\n",
    "                                 \"RowsWithMissingData\": missing_data_absolute,\n",
    "                                 \"PercentageOfDataset\": missing_data_percentage}\n",
    "                                  )\n",
    "                    .sort_values(by=['PercentageOfDataset'],ascending=False)\n",
    "                    .query(\"PercentageOfDataset > 0\")\n",
    "                    )\n",
    "\n",
    "  return df_missing_data"
   ]
  },
  {
   "source": [
    "We can apply this to our data set to check it works by comparing the results with the information given by panda's profile report."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualizeMissingData(df_prices)"
   ]
  },
  {
   "source": [
    "We will use a custom function presented in Feature Engine Unit 9 of the course in order to assess the effect of cleaning the data. This function can be used when the input mean, median or arbitrary number is a numerical variable and/or when we replace with 'Missing' or most frequent a categorical variable. The parameters are:\n",
    "        * df_original: data not cleaned\n",
    "        * df_cleaned: cleaned data\n",
    "        * variables_applied_with_method: variables where you applied a given method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def DataCleaningEffect(df_original,df_cleaned,variables_applied_with_method):\n",
    "\n",
    "  flag_count=1 # Indicate plot number\n",
    "  \n",
    "  # distinguish between numerical and categorical variables\n",
    "  categorical_variables = df_original.select_dtypes(exclude=['number']).columns \n",
    "\n",
    "  # scan over variables, \n",
    "    # first on variables that you applied the method\n",
    "    # if the variable is a numerical plot, a histogram if categorical plot a barplot\n",
    "  for set_of_variables in [variables_applied_with_method]:\n",
    "    print(\"\\n=====================================================================================\")\n",
    "    print(f\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\")\n",
    "    print(f\"{set_of_variables} \\n\\n\")\n",
    "  \n",
    "\n",
    "    for var in set_of_variables:\n",
    "      if var in categorical_variables:  # it is categorical variable: barplot\n",
    "        \n",
    "        df1 = pd.DataFrame({\"Type\":\"Original\",\"Value\":df_original[var]})\n",
    "        df2 = pd.DataFrame({\"Type\":\"Cleaned\",\"Value\":df_cleaned[var]})\n",
    "        dfAux = pd.concat([df1, df2], axis=0)\n",
    "        fig , axes = plt.subplots(figsize=(15, 5))\n",
    "        sns.countplot(hue='Type', data=dfAux, x=\"Value\",palette=['#432371',\"#FAAE7B\"])\n",
    "        axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.legend() \n",
    "\n",
    "      else: # it is numerical variable: histogram\n",
    "\n",
    "        fig , axes = plt.subplots(figsize=(10, 5))\n",
    "        sns.histplot(data=df_original, x=var, color=\"#432371\", label='Original', kde=True,element=\"step\", ax=axes)\n",
    "        sns.histplot(data=df_cleaned, x=var, color=\"#FAAE7B\", label='Cleaned', kde=True,element=\"step\", ax=axes)\n",
    "        axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
    "        plt.legend() \n",
    "\n",
    "      plt.show()\n",
    "      flag_count+= 1"
   ]
  },
  {
   "source": [
    "### Split data into train and test sets\n",
    "\n",
    "First we will split our data into train and test sets. This will allow us to see the results of our imputations on the test set after training on the train set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "TrainSet, TestSet, _, __ = train_test_split(\n",
    "                                        df_prices,\n",
    "                                        df_prices['SalePrice'],\n",
    "                                        test_size=0.2,\n",
    "                                        random_state=0)\n",
    "\n",
    "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "We can check our missing data levels in our train set are similar to the ones present in our full dataset. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "df_missing_data_trainset = VisualizeMissingData(TrainSet)\n",
    "print(f\"* There are {df_missing_data_trainset.shape[0]} variables with missing data \\n\")\n",
    "df_missing_data_trainset"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Now that we have explored our missing data in more detail, we can assess how we will deal with it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Variables with high levels of missing data\n",
    "\n",
    "* We will make the decision to drop the two variables with high levels of missing data (\"EnclosedPorch\" and \"WoodDeckSF\", with more than 89% of data missing). Such a high level of missing data means that the variables will not have any noticeable influence on our target variable. We will therefore save time by removing them from our cleaned data set early."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropFeatures\n",
    "imputer = DropFeatures(features_to_drop=['EnclosedPorch' , 'WoodDeckSF'])\n",
    "imputer.fit(TrainSet)\n",
    "df_method = imputer.transform(TrainSet)"
   ]
  },
  {
   "source": [
    "* We can check the effects of our imputation.\n",
    "    * We notice that the two features with more than 89% data missing have been dropped."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_data_trainset = VisualizeMissingData(df_method)\n",
    "print(f\"* There are {df_missing_data_trainset.shape[0]} variables with missing data \\n\")\n",
    "df_missing_data_trainset"
   ]
  },
  {
   "source": [
    "### Categorical data\n",
    "\n",
    "* We have two categorical variables that contain missing data. GarageFinish, representing the interior finish of the garage, with 11.22% of missing data. And BsmtFinType1, representing the rating of the basement finished area, with 7.62% of missing data.\n",
    "* We can deal with them in a few different ways:\n",
    "    * Delete the whole column. This would mean loosing a lot of data as we are only missing about 10% of our data and we could loose valuable insight.\n",
    "    * Replace the missing values with the most frecuent ones. This could be useful but further imbalance our dataset.\n",
    "    * Replace the missing values with the one of our choice.\n",
    "    * Delete rows with null values. This runs the risk of reducing our dataset\n",
    "    * Develop a model to predict the missing values.\n",
    "* Deleting the whole column would make us loose possible valuable insights as about 90% of our data is present. Replacing the missing values with the most frecuent ones risks imbalancing our dataset and affecting the influence of that category too much. Deleting rows with missing values will affect the overall size of our dataset. Developing a model would be the most accurate option, we might look into this option in further iterations of this project.\n",
    "* In this scenario we will choose the 3d option. We will use the categorical imputation to assign the category \"None\" to all missing data. This will allow us to quickly deal with low levels of missing data whilst not infuencing the correlation between existing data and the target variable. We will then use our DataCleaningEffect function to visualize the results of our imputation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "variables_categorical = ['GarageFinish' , 'BsmtFinType1']\n",
    "imputer = CategoricalImputer(imputation_method='missing', fill_value='None', variables=variables_categorical)\n",
    "df_method = imputer.fit_transform(TrainSet)\n",
    "DataCleaningEffect(df_original=TrainSet,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_categorical)"
   ]
  },
  {
   "source": [
    "### Numerical data\n",
    "\n",
    "* We have five numerical variables with missing data:\n",
    "    * LotFrontage, representing the linear feet of street connected to the property , with 18.15% of missing data.\n",
    "    * BedroomAbvGr, representing the bedrooms above grade (not including basement bedrooms) , with 6.85% of missing data.\n",
    "    * 2ndFlrSF, representing the second floor square footage, with 5.14% of missing data.\n",
    "    * GarageYrBlt, representing the year the garage was built, with 4.97% of missing data.\n",
    "    * MasVnrArea, representing the Masonry veneer area in square feet, with 0.51% of missing data.\n",
    "* As we are dealing with numerical values, we will use a Mean Median Imputer to replace missing data by the mean or median value of the variable. In order to assess if we will choose the mean or the median, we will see if the variables' distribution is normal."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['LotFrontage' , 'BedroomAbvGr', '2ndFlrSF', 'GarageYrBlt', 'MasVnrArea']:\n",
    "  sns.histplot(data=TrainSet, x=col, kde=True)\n",
    "  plt.show()"
   ]
  },
  {
   "source": [
    "* As we can see from these histogram plots, LotFrontage and BedroomAbvGr have a bell shaped distribution which indicates they are normally distributed and we can use mean imputation.\n",
    "* The rest of the variables, 2ndFlrSF, GarageYrBlt adn MasVnrArea are skewed, either to the right or to the left. We will therefore use the median as the mean is affected by outlier values and may not be as representative of our data.\n",
    "\n",
    "#### Mean imputation and visualisation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "variables_mean = ['LotFrontage' , 'BedroomAbvGr']\n",
    "imputer = MeanMedianImputer(imputation_method='mean', variables=variables_mean)\n",
    "df_method = imputer.fit_transform(TrainSet)\n",
    "DataCleaningEffect(df_original=TrainSet,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_mean)"
   ]
  },
  {
   "source": [
    "#### Median imputation and visualisation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_median = ['2ndFlrSF' , 'GarageYrBlt', 'MasVnrArea']\n",
    "imputer = MeanMedianImputer(imputation_method='median', variables=variables_median)\n",
    "df_method = imputer.fit_transform(TrainSet)\n",
    "DataCleaningEffect(df_original=TrainSet,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_median)"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Arrange cleaning steps in pipeline\n",
    "\n",
    "* We will arrange our data cleaning steps in a pipeline. This will facilitate the data cleaning of our whole dataset.\n",
    "* The steps are as follows:\n",
    "    * Dropping EnclosedPorch and WoodDeckSF variables.\n",
    "    * Categorical imputation on GarageFinish and BsmtFinType1 variables.\n",
    "    * Numerical mean imputation on LotFrontage and BedroomAbvGr variables.\n",
    "    * Numerical median imputation on 2ndFlrSF, GarageYrBlt and MasVnrArea variables."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "dataCleaning_pipeline = Pipeline([\n",
    "    \n",
    "      ( 'drop',  DropFeatures(features_to_drop=['EnclosedPorch', 'WoodDeckSF']) ),\n",
    "      ( 'categorical',  CategoricalImputer(imputation_method='missing',\n",
    "                                     fill_value='None',\n",
    "                                     variables=['GarageFinish' , 'BsmtFinType1']) ),\n",
    "      ( 'mean',  MeanMedianImputer(imputation_method='mean',\n",
    "                                     variables=['LotFrontage' , 'BedroomAbvGr']) ),\n",
    "      ( 'median',  MeanMedianImputer(imputation_method='median',\n",
    "                                     variables=['2ndFlrSF', 'GarageYrBlt', 'MasVnrArea']) )\n",
    "      \n",
    "])"
   ]
  },
  {
   "source": [
    "* We apply run the whole dataset through the pipeline to clean all our data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet, TestSet = dataCleaning_pipeline.fit_transform(TrainSet) , dataCleaning_pipeline.fit_transform(TestSet)\n",
    "df_prices = dataCleaning_pipeline.fit_transform(df_prices)"
   ]
  },
  {
   "source": [
    "* We confirm that there is no missing data left in our Train and Test set as well as our dataframe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualizeMissingData(TrainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualizeMissingData(TestSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualizeMissingData(df_prices)"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltNetd085qHf"
   },
   "source": [
    "# Push files to Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We push our Train, Test and cleaned data sets to the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKlnIozA4eQO",
    "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  os.makedirs(name='outputs/datasets/cleaned')\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)\n",
    "TestSet.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)\n",
    "df_prices.to_csv(\"outputs/datasets/cleaned/HousePricesCleaned.csv\", index=False)\n"
   ]
  },
  {
   "source": [
    "* And we save our pipeline in case of further use (such as cleaning the inherited houses data)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "file_path = f'outputs/ml_pipeline/data_cleaning'\n",
    "\n",
    "try:\n",
    "  os.makedirs(name=file_path)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(value=dataCleaning_pipeline, filename=f\"{file_path}/dataCleaning_pipeline.pkl\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}